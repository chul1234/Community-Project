의사결정 나무(Decision tree)
데이터를 분석 이들 사이에 존재하는 패턴을 예측 가능한 규칙들의 조합
질문을 던져서 대사을 좁혀가는 스무고개 같은 개념 ex) 아키네이터

트리구조
슬라이드 4
• 루트(root) 노드
: 주어진 그래프의 시작 노드로서,
통상 트리의 가장 높은 곳에 위치, 부모노드가 없다.
• 자손 노드(children node)
: 어떤 노드의 서브 트리의 루트 노드 ex) 2,3번 1번의 자식노드
• 부모 노드(parent node) 
: 자식 노드의 반대 개념 ex)1번은 2,3번이 부모노드
• 잎 노드(leaf node)
: 자손 노드가 없는 노드 
• 레벨(level)
: 루트의 레벨을 0으로 놓고, 자손 노드로 내려가면서 하나씩 증가 ex) 2,3 레벨1 4,5,6,7,8 레벨2
• *깊이(depth)*
: 트리에서 노드가 가질 수 있는 최대 레벨 ex) 깊이 3레벨

슬라이드5
lris dataset을 이용한 그래프
슬라이드6
구분을 위해 직선으로 구분하는것 .
슬라이드7
직선으로 petal length(cm) 2.45이하로 나눈다.
true면 setosa다
false이면 petal width 1.75이하다라는 노드를 새로 만든다
true이면 versicolor다(49,5)
false 이면 verginica다(1,45)

슬라이드 8
루트노드
petal width <= 0.75
true 면 setosa(50,0,0)
false면 새로운 노드 작성
petal width <1.75
true면 versicolor(0,49,5)
false면 virginica (0,1,45)
슬라이드 9
이런식으로 나눈다 라는것을 알면 된다.

슬라이드10
의사결정나무:
패턴을 예측 가능한 규칙들의 조합

슬라이드11
entropy
불순도 지표 : 나눴을때 퓨어 하냐? 안하냐? 한가지의 데이터만 있으면 표어 그 반대는 퓨어하지 않다.
퓨어인경우 : 100%
그렇지 않은경우 : 99%,97%

 슬라이드 19
아이의 행동을 데이터로 만듬
샘플 14개
yes 9
no 5
gini0.459
스플릿 1.outlook 3개(1. sunny, overcast, rain)
             sunny 5개 play yes 2개 no 3개 gini 0.48
        overcast yes 4개 no 0개 gini 0
        rain  yse 3개 no 2개 gini 0.48
        불순도 14개중에 5개 14분의 5 *0.48 +14분의 4*0+14분의 5 *0.48 
             (IG) 0.459-14분의 5 *0.48 +14분의 4*0+14분의 5 *0.48 
            2.temperature(hot,mild,cood)
         hot yes 2   no 2  gini 0.5
         mild yes 4  no 2  gini 0.444
         cood yes 3 no 1  gini 0.375
         불순도(IG)0.459- 14분의 4*0.5+14분의6*0.444+14분의4*0.375 = 0.020
0.459- 14분의 4*0.5+14분의6*0.444+14분의4*0.375 = 0.020
       3.humidity
         high yes 3 no 4 gini 0.490
         normal yes 6 no 1 gini 0.245
         불순도 0.045-14분의7*0.490+14분의 7*0.245 = 0.092
ig0.045-14분의7*0.490+14분의 7*0.245 = 0.092
       4.windy
         불떄 yes 3 no 3 0.5
         안불때 yes 6 no 2 0.375
         불순도 14분의 6*0.5+14분의8*0.375
ig(infomation gain) = 0.030
종합 outlook 이 좋다.(의사결정나무를 만들면 된다.)
불순해서 좀더 퓨어하게 하기 위해 한번더 한다.
lv1
sunny yes 2 no 3 gini0.48
1. temperature  hot , mild, cood
   hot yes 0 no 2 gini 0
   mild yes 1 no 1 gini 0.5
   cood yes 1 no 0  gini 0
   불순도 5분의 2 *0+5분의 2*0.5+5분의 2+0=0.2
temp 의 ig= 0.28
2. humidity high normal
  high yes 0 no 3  gini 0   
  normal yes 2 no 0  gini 0   
  불순도 0
humildity ig= 0.48
wild의 ig=0.014
그럼 humildity가 된다.
rain도 이런식으로 해야한다.
lv2
깊이 레벨2

엔트로피 gini는 노드의 불순도 계산법이다
ig는 부모노드-자식노드 불순도 차이
부모의 노드와 자식노드만 신경쓰면된다
ig가 큰값을 중심으로 또 자식노드를 만든다.

의사결정나무는greedy algorithm사용
greedy algorithm(다음단계를 고려하지 않고 지금당장의 최적해를 구함)
최적해를 구하는 방법 중 하나
여러 경우 중 하나선택 해당 상황에서 가장 좋은것을 선택(top-down 방식)
납득할 만한 훌륭한 솔루션을 도출할 수 있다. 최적의 솔루션은 아님

white box model(보고 이해가능)위 슬레이드가 예시 
간단하고 명확한 분류 방법 사용(의사결정나무 장점)
직관적, 결정방식 이해가 쉬움(의사결정나무 장점)
반대 black box은 훈련된 모델이 왜 그런 예측을 만드는지 설명하기 어려움(봐도 이해불가능)
black box(인공신경망, 랜덤포레스트)

카테고리(연속)
컨티니어서(범위)

슬라이드20 (알고리즘, 특징등 알고만 있어라)
지금 수업에서는 cart를 사용
cart는 회귀랑 분류에 사용가능하다.

슬라이드 22
비 모수적
의사결정 나무 (파라미터 스플릿과 스플릿 값)
차이:1. 파라미터 계수가 모수는 고정 비모수적 모델 파라미터 계수 데이터에 따라서 증가
     2. 모델 복잡도 모수적 정해져있다. 비 모수적은 한계가 있다. 
        3. 비모수적은 overfitting 모수적 underfitting
        4.  비모수적 의사결정 나무 모수적 linear regression 

슬라이드 23
의사결정 나무 단점
중간에 한번 망하면 아래도 다 망한다.
노이즈에 매우 민감
과적합 위험
높은 예측 성능을 갖는데 다소 부족

슬라이드 24 
규제가 없으면 이런식으로 나온다.

슬라이드25
제한 방법
1. 가지치기 
    다 키우고 불필요한 가지를 자른다.
    필요없는 부분 정한다
   
2. 사이킥런에 있는 법
    클래스 사용해서 한 것 
max_depth 최대 깊이를 제한
min_samples_split 노드가 있을때 샘플의 스플릿을 하는데 최소값 (최소2개)
min_samples_leaf 잎 노드의 최소 샘플 갯수(만약 크면 갯수가 적으면 멈춘다)
위에것들은 하이퍼 파라미터이다.






